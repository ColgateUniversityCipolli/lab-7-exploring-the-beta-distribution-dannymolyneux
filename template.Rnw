\documentclass{article}
\usepackage{amsmath} %This allows me to use the align functionality.
                     %If you find yourself trying to replicate
                     %something you found online, ensure you're
                     %loading the necessary packages!
\usepackage{amsfonts}%Math font
\usepackage{graphicx}%For including graphics
\usepackage{hyperref}%For Hyperlinks
\usepackage[shortlabels]{enumitem}% For enumerated lists with labels specified
                                  % We had to run tlmgr_install("enumitem") in R
\hypersetup{colorlinks = true,citecolor=black} %set citations to have black (not green) color
\usepackage{natbib}        %For the bibliography
\setlength{\bibsep}{0pt plus 0.3ex}
\bibliographystyle{apalike}%For the bibliography
\usepackage[margin=0.50in]{geometry}
\usepackage{float}
\usepackage{multicol}

%fix for figures
\usepackage{caption}
\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}
\begin{document}

\vspace{-1in}
\title{Lab 7/8 -- MATH 240 -- Computational Statistics}

\author{
  Danny Molyneux \\
  Colgate University  \\
  Mathematics  \\
  {\tt dmolyneux@colgate.edu}
}


\maketitle

\begin{multicols}{2} \raggedcolumns
\begin{abstract}
In labs 7 and 8, I did a thorough analysis of the beta distribution. This includes an analysis of its parameters, properties, what it looks like, and what it's used for. This information is helpful for the same reason it would be helpful for any probability distribution. They are a way for us to estimate various properties of data that approximately follows a certain probability distribution.
\end{abstract}

\noindent \textbf{Keywords:} Probability, moments, summarize, sample size.

\section{Introduction}
In this lab, there is not one specific question we want to answer. We more so want to analyze the beta distribution by looking at its use, meaning, parameters, properties, and what it looks like.

We analyze the distribution and its parameters by using multiple sets of parameters, and comparing their plots as well as their numerical summaries. To analyze the properties of the beta distribution, we first see how the mean, variance, skew, and kurtosis vary based on the sample size, and whether or not they converge to the true population values. If you want to see the distribution of these statistics, called sampling distributions, we can do what's called resampling, and store each statistic for every sample we take. 

Another question we aim to answer at the end of the lab is which estimators we should use for the beta distribution, method of moments, or maximum likelihood estimates? We can do this through the use of sampling distributions again, but now for the MOM and MLE for each new sample. I used data on country death rates from 2022 as an example. Calculating properties like bias, precision, and mean squared error can tell us which estimator is more optimal. 

\section{Density Functions and Parameters}
The beta distribution is a continuous distribution that is used to model a random variable that ranges from 0 to 1, meaning it good at modeling probabilities/proportions. It has two parameters, alpha and beta, which are both positive numbers. These parameters affect the distribution's shape, making it very flexible. Let's look more closely at what each of these parameters really means, and what this distribution looks like generally.
<<size = 'scriptsize', echo=FALSE, fig.width = 6, fig.height=4, results="asis", warning = F, message = F>>=
library(tidyverse)
library(patchwork)
#function plots the beta distribution for alpha, beta
beta.plot = function(alpha, beta){
  beta.dat <- tibble(x = seq(-0.25, 1.25, length.out=1000))|> # generate a grid of points
    mutate(beta.pdf = dbeta(x, alpha, beta))
  distribution = ggplot(data= beta.dat)+  # specify data
    geom_line(aes(x=x, y=beta.pdf, color=paste("Beta(", alpha, ", ", beta, ")", sep = ""))) +
    geom_hline(yintercept=0)+
    theme_bw()+
    xlab("x")+
    ylab("Density")+
    labs(color = "")
  return(distribution)
}
pdf("beta.plots.pdf", width = 6, height = 4)
beta.plot(2,5) + beta.plot(5,5) + beta.plot(5,2) + beta.plot(0.5,0.5)
dev.off()
@
\begin{figure} [H]
\centering
\includegraphics[width=\linewidth]{beta.plots.pdf}
\caption{Histograms for various beta distributions}
\label{fig:beta}
\end{figure}

\begin{table}[H]
\centering
\resizebox{0.4\textwidth}{!}{ 
  \begin{tabular}{rllllll}
    \hline
  & alpha & beta & mean & variance & skew & kurt \\ 
    \hline
  1 & 2.0 & 5.0 & 0.2857143 & 0.02551020 & 0.5962848 & -0.1200000 \\ 
  2 & 5.0 & 5.0 & 0.5000000 & 0.02272727 & 0.0000000 & -0.4615385 \\ 
  3 & 5.0 & 2.0 & 0.7142857 & 0.02551020 & -0.5962848 & -0.1200000 \\ 
  4 & 0.5 & 0.5 & 0.5000000 & 0.12500000 & 0.0000000 & -1.5000000 \\
    \hline
  \end{tabular}
}
\caption{Numerical summaries of beta distributions} 
\label{beta.plots:reference}
\end{table}

As you can see, I made plots for each of the four distributions using the ggplot2 *** and tidyverse *** packages. The Beta(2,5) plot is skewed left, and the Beta(5,2) plot is the same graph, except skewed right. When alpha and beta are equal (5,5), the plot is symmetrical. This tells me that having a larger alpha skews the graph right, and a larger beta skews the graph left. What this means is that a larger alpha means more data will be near 1, indicating that alpha represents "successes". We can say the opposite about beta. However, you can see in the Beta(0.5,0.5) plot that it has a U-shape. This is because when the parameters are less than 1, the plot becomes bimodal with peaks near 0 and 1. This is just a result due to the probability density function of the Beta distribution. One thing to note from the table if you couldn't tell from the plots is that the mean is 0.5 if alpha and beta are equal. If alpha is larger, the mean is closer to one, and the opposite is true if beta is larger. This makes sense given the skewness we know that the parameters cause.

So now we know that if the two parameters are greater than or equal to one, the Beta distribution will have a bell-shaped curve, with its skewness dependent on which parameter is larger. If alpha is larger, it will have a right skew, and vice versa for beta. If they are equal, it will be symmetric. In the case that the parameters are less than one, the plot will take on a U-shaped curve, with peaks near zero and one. The closer the parameters are to zero, the higher the density at the ends will be. We also saw some patterns in the statistics in relation to the parameters, but we will get more into that in the next section.

\section{Properties (Mean, variance, skewness, and kurtosis)}
There are formulas with respect to alpha and beta for the mean, variance, skewness, and kurtosis of the beta distribution. Another way to caluclate these statistics is to use moments (centered and uncentered). We can then compare these results to the values we got from the original formulas to confirm that our function is working properly. In order to calcualte these statistics using moments, I used the \texttt(integrate()) function from base \texttt{R}.

One interesting thing to look at in regards to these statistics, is how their variance changes based on sample size. Is the sample skewness always going to be close to the true population skewness? If not, how big of a sample size does it take to converge? Well we can answer this question by computing the cumulative numerical summaries using the cumstats **** package, while also using \texttt{geom\textunderscore line()} to plot the true population values. The patchwork *** package is also useful here so that we can plot all four statistics next to each other.

<<size = 'scriptsize', echo=FALSE, fig.width = 6, fig.height=4, results="asis", warning = F, message = F>>=
library(cumstats)
library(patchwork)
data_2_5 = beta.data[1, ] #true population values for beta(2, 5) (first row of task one data frame)
alpha = 2
beta = 5
n = 500
set.seed(7272)
sample.dat = rbeta(n, alpha, beta) #sample data
#view(sample.dat)
#cumulative variables
cum.mean = cummean(sample.dat)
cum.var = cumvar(sample.dat)
cum.skew = cumskew(sample.dat)
cum.kurt = cumkurt(sample.dat)
#data frame for the cumulative numerical summary
cum.dat = tibble(
  data.points = 1:n,
  mean = cum.mean,
  variance = cum.var,
  skewness = cum.skew,
  kurtosis = cum.kurt
)
#view(data_2_5)
#plots for each cumulative variable
mean.plot = ggplot(cum.dat, aes(x=data.points, y = mean)) +
  geom_line() +
  geom_hline(yintercept = data_2_5$mean, linetype = "dashed", color = "blue") +
  labs(title = "Cumulative mean for a Beta(2,5) sample", x = "# of data points", y = "Cumulative Mean")

var.plot = ggplot(cum.dat, aes(x=data.points, y = variance)) +
  geom_line() +
  geom_hline(yintercept = data_2_5$variance, linetype = "dashed", color = "blue") +
  labs(title = "Cumulative variance for a Beta(2,5) sample", x = "# of data points", y = "Cumulative Variance")

skew.plot = ggplot(cum.dat, aes(x=data.points, y = skewness)) +
  geom_line() +
  geom_hline(yintercept = data_2_5$skew, linetype = "dashed", color = "blue") +
  labs(title = "Cumulative skewness for a Beta(2,5) sample", x = "# of data points", y = "Cumulative Skewness")

kurt.plot = ggplot(cum.dat, aes(x=data.points, y = kurtosis)) +
  geom_line() +
  #adjust kurtosis by 3 because cumstats package uses regular kurtosis
  geom_hline(yintercept = data_2_5$kurt + 3, linetype = "dashed", color = "blue") +
  labs(title = "Cumulative kurtosis for a Beta(2,5) sample", x = "# of data points", y = "Cumulative Kurtosis")
pdf("cum.plot.pdf", width = 4, height = 2)
mean.plot + var.plot + skew.plot + kurt.plot
dev.off()
@
\begin{figure} [H]
\centering
\includegraphics[width=\linewidth]{/Users/dannymolyneux/Documents/GitHub/lab-7-exploring-the-beta-distribution-dannymolyneux/cum.plot.pdf}
\caption{Cumulative stats for Beta(2, 5)}
\label{fig:cumstats}
\end{figure}

As you can see in the plots, the cumulative statistics do end up converging to their true values, but it definitely takes time. For mean and kurtosis, I would say after 100 data points, the cumulative statistic stays pretty close to its true value. Variance actually stays above its true value even after 500 data points, so maybe we needed more data for variance. Skewness looks like it is converging very quickly (around 30 data points), but then goes away from its true value until around 300 data points. 

If we want to see the distribution of all of these different statistics, we can use the resampling method I mentioned earlier. I stored each statistic for 1000 different samples, and then plotted the distribution for each stat.

<<size = 'scriptsize', echo=FALSE, fig.width = 6, fig.height=4, results="asis", warning = F, message = F>>=
sample.distribution = data.frame() #data frame that will have all 1000 samples
for(i in 1:1000){
  set.seed(7272+i)
  beta.sample <- rbeta(n,  # sample size
                       alpha,   # alpha parameter
                       beta)    # beta parameter
  beta.sample = data.frame(x = beta.sample) #turn it into a data frame
  new.sample.summary = beta.sample |> #numerical summary for each sample
    summarize(
      alpha = alpha, beta = beta, mean = mean(x), variance = var(x), skew = skewness(x), kurt = kurtosis(x)-3
    )
  #add the data for that sample
  sample.distribution = bind_rows(sample.distribution, new.sample.summary)
}
#view(sample.distribution)
#Distribution plots for each variable
mean.dist.plot = ggplot(data= sample.distribution, aes(x=mean))+                 
  geom_histogram(aes(y=after_stat(density)), 
                 #binwidth = 0.1
                 #bins=10
                 breaks=seq(0.1,0.4,0.005)) +                 
  geom_hline(yintercept=0)+                                            
  theme_bw()+                                                       
  geom_density(color = "blue") +
  xlab("Mean")+                                                        
  ylab("Density")+                                             
  labs(title = "Distribution of Mean")
var.dist.plot = ggplot(data= sample.distribution, aes(x=variance))+                 
  geom_histogram(aes(y=after_stat(density)), 
                 #binwidth = 0.1
                 #bins=10
                 breaks=seq(0,0.05,0.0025)) +                 
  geom_hline(yintercept=0)+                                            
  theme_bw()+                                                       
  geom_density(color = "blue") +
  xlab("Variance")+                                                        
  ylab("Density")+                                             
  labs(title = "Distribution of Variance")
skew.dist.plot = ggplot(data= sample.distribution, aes(x=skew))+                 
  geom_histogram(aes(y=after_stat(density)), 
                 #binwidth = 0.1
                 #bins=10
                 breaks=seq(0,1,0.1)) +                 
  geom_hline(yintercept=0)+                                            
  theme_bw()+                                                       
  geom_density(color = "blue") +
  xlab("Skewness")+                                                        
  ylab("Density")+                                             
  labs(title = "Distribution of Skewness")
kurt.dist.plot = ggplot(data= sample.distribution, aes(x=kurt))+                 
  geom_histogram(aes(y=after_stat(density)), 
                 #binwidth = 0.1
                 #bins=10
                 breaks=seq(-4,-2,0.1)) +                 
  geom_hline(yintercept=0)+                                           
  theme_bw()+                                                       
  geom_density(color = "blue") +
  xlab("Excess Kurtosis")+                                                        
  ylab("Density")+                                             
  labs(title = "Distribution of Excess Kurtosis")
#I notice that these variables follow a relatively normal distribution, especially the 
#skewness and excess kurtosis. Obviously they aren't perfect because we are using samples,
#but you can see that their density lines have the bell curve shape
pdf("resampling.pdf", width = 4, height = 2)
mean.dist.plot + var.dist.plot + skew.dist.plot + kurt.dist.plot
dev.off()
@
\begin{figure} [H]
\centering
\includegraphics[width=\linewidth]{resampling.pdf}
\caption{Histograms for sampling distributions}
\label{fig:sampling}
\end{figure}

As you can see, and maybe expected, each statistic follows a relatively normal distribution.

\section{Estimators (MOM and MLE)}


\section{Example (Death rates data)}


 You should objectively evaluate the evidence you found in the data. Do not embellish or wish-terpet (my made-up phase for making an interpretation you, or the researcher, wants to be true without the data \emph{actually} supporting it). Connect your findings to the existing information you provided in the Introduction.

Finally, provide some concluding remarks that tie together the entire paper. Think of the last part of the results as abstract-like. Tell the reader what they just consumed -- what's the takeaway message?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{2em}

\noindent\textbf{Bibliography:} Note that when you add citations to your bib.bib file \emph{and}
you cite them in your document, the bibliography section will automatically populate here.

\begin{tiny}
\bibliography{bib}
\end{tiny}
\end{multicols}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\onecolumn
\section{Appendix}

If you have anything extra, you can add it here in the appendix. This can include images or tables that don't work well in the two-page setup, code snippets you might want to share, etc.

\end{document}